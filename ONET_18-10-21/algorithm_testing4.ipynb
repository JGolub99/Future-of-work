{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e133cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This version reframes the problem as a regression case'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Skills.xlsx\")\n",
    "#print(df.head())\n",
    "\n",
    "#Lets remove the importance values:\n",
    "\n",
    "df2 = df.loc[df[\"Scale Name\"] == \"Level\"]\n",
    "df2.reset_index(drop = True, inplace = True)\n",
    "#print(df2.head())\n",
    "\n",
    "#Lets now remove the irrelevent columns:\n",
    "\n",
    "df3 = df2.drop(columns = [\"Scale ID\",\"Scale Name\",\"N\",\"Recommend Suppress\",\"Not Relevant\",\"Date\",\"Domain Source\"])\n",
    "print(df3.head())\n",
    "\n",
    "#NOTE that we have ignored the suppress recomendations, we shall continue with this for now but will need to address this later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420df498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now discover a bit about our data set:\n",
    "\n",
    "df3.info()\n",
    "\n",
    "#We note that there are some occupations for which the standard error and bound values are missing. Lets supress these for now:\n",
    "\n",
    "df3.drop(columns = [\"Standard Error\",\"Lower CI Bound\",\"Upper CI Bound\"],inplace = True)\n",
    "print(df3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a1537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I shall implement the drop_duplicates method:\n",
    "\n",
    "df4 = df3[[\"O*NET-SOC Code\",\"Title\"]]\n",
    "df4.drop_duplicates(inplace=True)\n",
    "df4.reset_index(drop = True, inplace = True)\n",
    "#print(df4.head())\n",
    "\n",
    "#We now need to add the variables. Begin by adding empty columns to the dataframe:\n",
    "\n",
    "n_jobs = len(set((df3[\"Title\"])))\n",
    "n_variables = len(set((df3[\"Element Name\"])))\n",
    "\n",
    "for i in range(n_jobs):\n",
    "    df4[df3[\"Element Name\"][i]] = \"\"\n",
    "\n",
    "#print(df4.head())\n",
    "#Now we need to fill these columns:\n",
    "\n",
    "x = df3.loc[df3[\"Title\"] == \"Chief Executives\"]\n",
    "y = x[\"Data Value\"]\n",
    "\n",
    "for i in range(n_variables):\n",
    "    df4[df4.columns[2+i]][0] = y[i]\n",
    "    \n",
    "#We now need to do this procedure for every job:\n",
    "\n",
    "for j in range(n_jobs):\n",
    "    x = df3.loc[df3[\"Title\"] == df4.iloc[j,1]]\n",
    "    y = x[\"Data Value\"]\n",
    "    y.reset_index(drop = True, inplace = True)\n",
    "    for i in range(n_variables):\n",
    "        df4[df4.columns[2+i]][j] = y[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0748248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df4.head())\n",
    "print(df4.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194be949",
   "metadata": {},
   "source": [
    "### We now have the skills dataframe just as we want it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edae9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets understand our data a bit:\n",
    "\n",
    "df4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "hist = df4.iloc[:,20].hist(bins=20)\n",
    "\n",
    "#We can inspect the histogram of any variable we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628f6e5",
   "metadata": {},
   "source": [
    "### Let's now  import another dataframe with autovalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87641e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_excel(\"US_data_email.xls\")\n",
    "df5 = df5[[\"Occupation Name\",\"BLS codes\",\"Training set automatable labels\"]]\n",
    "print(df5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40868f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422495f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets define a function so that we can make occupation ID's consistent:\n",
    "\n",
    "def title_set(my_string):\n",
    "    my_list = []\n",
    "    my_list[:0] = my_string\n",
    "    my_list.remove(\"_\")\n",
    "    my_list.append(\".00\")\n",
    "    my_output = \"\".join(my_list)\n",
    "    return my_output\n",
    "\n",
    "#print(title_set(\"45-4023_\"))\n",
    "\n",
    "df5.iloc[:,1] = df5.iloc[:,1].apply(title_set)\n",
    "print(df5.head())\n",
    "df5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88670d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now concatenate the auto labels to our 1st dataframe:\n",
    "import numpy as np\n",
    "df4[\"Auto label value\"] = np.nan\n",
    "for i in list(df5[\"BLS codes\"]):\n",
    "    df4.loc[df4[\"O*NET-SOC Code\"] == i,\"Auto label value\"] = list(df5.loc[df5[\"BLS codes\"] == i,\"Training set automatable labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.info()\n",
    "\n",
    "#Note - the auto value count is supposedly 330 non-null, even though it should be 70. After creating a csv from the dataframe,\n",
    "#I found that there were 70 non-null as expected. Worth bringing up with Mike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035d0d9",
   "metadata": {},
   "source": [
    "### START FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec8cea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.298618</td>\n",
       "      <td>7.990448</td>\n",
       "      <td>2.058391</td>\n",
       "      <td>1.077975</td>\n",
       "      <td>0.396031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.855392</td>\n",
       "      <td>7.037031</td>\n",
       "      <td>6.702554</td>\n",
       "      <td>3.942593</td>\n",
       "      <td>1.168411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.033326</td>\n",
       "      <td>3.936282</td>\n",
       "      <td>6.937451</td>\n",
       "      <td>9.739695</td>\n",
       "      <td>-1.108691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.914224</td>\n",
       "      <td>3.498826</td>\n",
       "      <td>9.307832</td>\n",
       "      <td>6.318492</td>\n",
       "      <td>-1.209890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.659548</td>\n",
       "      <td>6.087176</td>\n",
       "      <td>3.198000</td>\n",
       "      <td>4.393357</td>\n",
       "      <td>-0.709211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3    Output\n",
       "0  8.298618  7.990448  2.058391  1.077975  0.396031\n",
       "1  8.855392  7.037031  6.702554  3.942593  1.168411\n",
       "2  1.033326  3.936282  6.937451  9.739695 -1.108691\n",
       "3  4.914224  3.498826  9.307832  6.318492 -1.209890\n",
       "4  3.659548  6.087176  3.198000  4.393357 -0.709211"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df4 = pd.read_csv(\"generated_data2.csv\")\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2d9a8",
   "metadata": {},
   "source": [
    "### We now have a dataset which encompass jobs titles, SOC codes, skill levels and hand picked auto labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d2e87",
   "metadata": {},
   "source": [
    "### Lets now split and standardize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To prevent information about the distribution of the test set leaking into the model, we shall first form a training set\n",
    "# and form a scaler operator from this, and then apply this to both training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fc4bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       200 non-null    float64\n",
      " 1   1       200 non-null    float64\n",
      " 2   2       200 non-null    float64\n",
      " 3   3       200 non-null    float64\n",
      " 4   Output  200 non-null    float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 7.9 KB\n"
     ]
    }
   ],
   "source": [
    "#Lets now create a training set which includes only the jobs for which we have hand picked auto values:\n",
    "\n",
    "training_set = df4.dropna(axis=0,how=\"any\")\n",
    "training_set.reset_index(drop = True, inplace=True)\n",
    "#print(training_set.head())\n",
    "training_set.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c955b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets apply stratified sampling on this set to create a training and test set\n",
    "#Code taken from Hands on Machine Learning book\n",
    "\n",
    "strat_train_set = df4.head(int(200*0.8))\n",
    "strat_test_set = df4.tail(int(200*0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04871038",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set.reset_index(drop=True,inplace=True)\n",
    "strat_test_set.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4985df74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Now that we have our training set, lets create a standardiser for it:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=True,with_std=True)\n",
    "scaler.fit(strat_train_set.iloc[:,0:4])\n",
    "scaled_training_values = scaler.transform(strat_train_set.iloc[:,0:4])\n",
    "scaled_test_values = scaler.transform(strat_test_set.iloc[:,0:4])\n",
    "scaled_train_set = strat_train_set.copy()\n",
    "scaled_test_set = strat_test_set.copy()\n",
    "#print(strat_train_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85fb4ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary = pd.DataFrame(data=scaled_training_values)\n",
    "temporary2 = pd.DataFrame(data=scaled_test_values) #we create temporary data frames from the numpy arrays we've just created\n",
    "#print(temporary)\n",
    "for i in range(0,4):\n",
    "    scaled_train_set[scaled_train_set.columns[i]] = temporary[temporary.columns[i]]\n",
    "    scaled_test_set[scaled_test_set.columns[i]] = temporary2[temporary2.columns[i]]\n",
    "\n",
    "#print(scaled_test_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0efe1133",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' We shall also create a standardized set for ALL of the training data '''\n",
    "\n",
    "scaler2 = StandardScaler(with_mean=True,with_std=True)\n",
    "scaler2.fit(strat_train_set.iloc[:,0:4])\n",
    "scaled_total_values = scaler2.transform(training_set.iloc[:,0:4])\n",
    "scaled_total_set = training_set.copy()\n",
    "\n",
    "temporary3 = pd.DataFrame(data=scaled_total_values)\n",
    "for i in range(0,4):\n",
    "    scaled_total_set[scaled_total_set.columns[i]] = temporary3[temporary3.columns[i]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d735dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3    Output\n",
      "0  1.119159  0.953454 -1.140474 -1.835128  0.396031\n",
      "1  1.339266  0.593718  0.534637 -0.701906  1.168411\n",
      "2 -1.752994 -0.576233  0.619362  1.591384 -1.108691\n",
      "3 -0.218777 -0.741290  1.474339  0.237982 -1.209890\n",
      "4 -0.714782  0.235326 -0.729426 -0.523587 -0.709211\n",
      "          0         1         2         3    Output\n",
      "0 -0.030417  0.627923 -1.065881 -0.599325  0.503844\n",
      "1  1.763522  0.274512 -1.426195  1.631039  0.502226\n",
      "2  1.083760 -1.536759  1.490892 -1.014764 -0.432081\n",
      "3 -0.812005 -1.236787 -1.257394  1.096295 -1.796756\n",
      "4 -1.112818  0.098826  0.754426  0.356263 -0.532768\n",
      "          0         1         2         3    Output\n",
      "0  1.119159  0.953454 -1.140474 -1.835128  0.396031\n",
      "1  1.339266  0.593718  0.534637 -0.701906  1.168411\n",
      "2 -1.752994 -0.576233  0.619362  1.591384 -1.108691\n",
      "3 -0.218777 -0.741290  1.474339  0.237982 -1.209890\n",
      "4 -0.714782  0.235326 -0.729426 -0.523587 -0.709211\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train_set.head())\n",
    "print(scaled_test_set.head())\n",
    "print(scaled_total_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67dc1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_total_set.to_csv(\"PIGEBAQ_testset.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5c7ed",
   "metadata": {},
   "source": [
    "### We now have a fully scaled training and test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78aad85",
   "metadata": {},
   "source": [
    "### We can now perform Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfe4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We begin by creating a centred training set:\n",
    "\n",
    "X = strat_train_set.drop([\"Title\",\"O*NET-SOC Code\"],axis=1)\n",
    "\n",
    "#We now use the Scikit learn toolkit to visualise how the explained variance ratio changes with no. dimensions:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "dim = range(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(dim),np.array(cumsum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Having visualized the effect of dimensionality, we can implement this to our dataset:\n",
    "\n",
    "pca2 = PCA(n_components=0.95)\n",
    "X_reduced = pca2.fit_transform(X)\n",
    "print(X_reduced[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badabc02",
   "metadata": {},
   "source": [
    "### We shall now fit a GP classifier to the unreduced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30b04605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f905fc7203724593b4b6638707457670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(IntProgress(value=0, max=1000), HTML(value=''))), Box(children=(HTML(value=''),)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\GPy\\kern\\src\\stationary.py:168: RuntimeWarning:overflow encountered in true_divide\n",
      " C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\GPy\\kern\\src\\rbf.py:52: RuntimeWarning:overflow encountered in square\n",
      " C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\GPy\\kern\\src\\rbf.py:76: RuntimeWarning:invalid value encountered in multiply\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<paramz.optimization.optimization.opt_lbfgsb at 0x11dba7ca8e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Begin by creating numpy arrays for our input X and output Y:\n",
    "\n",
    "#X = np.array([scaled_train_set.iloc[:,0:4]])\n",
    "#Y = np.array([scaled_train_set.iloc[:,4]])\n",
    "X = np.array([scaled_total_set.iloc[:,0:4]])\n",
    "Y = np.array([scaled_total_set.iloc[:,4]])\n",
    "#X = np.transpose(X)\n",
    "#Y = np.transpose(Y)\n",
    "\n",
    "\n",
    "X = np.reshape(X,(200,4)) #Reshape to go from 3d matrix to 2d\n",
    "Y = np.reshape(Y,(200,1)) # ^\n",
    "\n",
    "#Now generate a kernel:\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "import GPy\n",
    "\n",
    "kernel = GPy.kern.RBF(input_dim=4, variance=100., lengthscale=100.)\n",
    "m_gpy = GPy.models.GPRegression(X,Y,kernel)\n",
    "m_gpy.optimize(messages=True)\n",
    "#m_gpy.optimize_restarts(num_restarts = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5691efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the rbf.variance?1.02\n",
      "What is the rbf.lengthscale?8.06\n"
     ]
    }
   ],
   "source": [
    "#We shall request values for the variance and lengthscale:\n",
    "\n",
    "m_var = input(\"What is the rbf.variance?\")\n",
    "m_length = input(\"What is the rbf.lengthscale?\")\n",
    "\n",
    "m_var = float(m_var)\n",
    "m_length = float(m_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e436ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate this model in scikit learn\n",
    "\n",
    "\n",
    "sci_kernel = m_var * RBF(m_length)\n",
    "gpc = GaussianProcessRegressor(kernel=sci_kernel,optimizer=None).fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de51ce",
   "metadata": {},
   "source": [
    "### Lets now apply k-fold cross validation on the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56997c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(scaled_train_set.iloc[:,0:4])\n",
    "#X_train = np.transpose(X_train)\n",
    "y_train = np.array(scaled_train_set.iloc[:,4])\n",
    "#y_train = np.transpose(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6974f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train,y_train):\n",
    "    clone_gpc = clone(gpc)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train[test_index]\n",
    "    \n",
    "    clone_gpc.fit(X_train_folds,y_train_folds)\n",
    "    y_pred = clone_gpc.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct/len(y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68786926",
   "metadata": {},
   "source": [
    "### What about an F1 score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185668f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_train_pred = cross_val_predict(gpc,X_train,y_train,cv=5)\n",
    "f1_score(y_train,y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c6f32",
   "metadata": {},
   "source": [
    "### And how about an AUC value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40910d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calc_AUC(gpc,X_train,y_train):\n",
    "    y_probas = cross_val_predict(gpc,X_train,y_train,cv=5,method=\"predict_proba\")\n",
    "    y_scores = y_probas[:,1]\n",
    "    return roc_auc_score(y_train,y_scores)\n",
    "\n",
    "calc_AUC(gpc,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ddfdf",
   "metadata": {},
   "source": [
    "### And a log-likelihood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "424fa99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-146596758938.59927"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpc.log_marginal_likelihood(theta=None, eval_gradient=False, clone_kernel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d78e0",
   "metadata": {},
   "source": [
    "### It is worth using the model to predict values for the test set to ensure it is working as I want it to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4d045d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(scaled_test_set.iloc[:,0:4])\n",
    "y_test = np.array(scaled_test_set.iloc[:,4])\n",
    "\n",
    "y_pred = gpc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd5b80",
   "metadata": {},
   "source": [
    "### We now have a fully working GP classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52436a7e",
   "metadata": {},
   "source": [
    "### Lets now consider the interpretability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c840583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Features  Importances\n",
      "3        3     1.283524\n",
      "2        2     1.312723\n",
      "0        0     1.404821\n",
      "1        1     1.758038\n"
     ]
    }
   ],
   "source": [
    "#We shall use the feature permutation method:\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "feature_importance = pd.DataFrame({\"Features\":np.array(training_set.columns[0:4])})\n",
    "\n",
    "r = permutation_importance(gpc,X_test,y_test,n_repeats=30,random_state=0)\n",
    "feature_importance[\"Importances\"] = abs(r.importances_mean)\n",
    "feature_importance = feature_importance.sort_values(by=['Importances'])\n",
    "#feature_importance = feature_importance\n",
    "print(feature_importance)\n",
    "#print(r.importances_mean)\n",
    "\n",
    "#for i in r.importances_mean.argsort()[::-1]:\n",
    "#    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "#        print(f\"{feature_importance.Features[i]:<8}\"\n",
    "#        f\"{r.importances_mean[i]:.3f}\"\n",
    "#        f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb49c78",
   "metadata": {},
   "source": [
    "### Now that we have the feature importances, lets calculate the entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ce2fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3727959816359824\n"
     ]
    }
   ],
   "source": [
    "# For the moment we won't normalise the distribution - might have to do this in the future (ask Mike)\n",
    "\n",
    "def log_calc(my_list):    #This function deals with values of 0\n",
    "    my_output = [0]*len(my_list)\n",
    "    for i in range(len(my_list)):\n",
    "        if my_list[i] != 0.0:\n",
    "            my_output[i] = np.log(my_list[i])\n",
    "    return my_output        \n",
    "            \n",
    "temp = abs(r.importances_mean)\n",
    "tempnew = temp/sum(temp)\n",
    "vector1 = np.array(tempnew)\n",
    "vector2 = np.array(log_calc(abs(r.importances_mean)))\n",
    "entropy = -1*np.dot(vector1,vector2)\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16486e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3727959816359824"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets create a function that does all of this:\n",
    "\n",
    "def calc_entropy(gpc,X_test,y_test):\n",
    "    r = permutation_importance(gpc,X_test,y_test,n_repeats=30,random_state=0)\n",
    "    temp = abs(r.importances_mean)\n",
    "    tempnew = temp/sum(temp)\n",
    "    vector1 = np.array(tempnew)\n",
    "    vector2 = np.array(log_calc(abs(r.importances_mean)))\n",
    "    return -1*np.dot(vector1,vector2)\n",
    "\n",
    "calc_entropy(gpc,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d928b",
   "metadata": {},
   "source": [
    "### Lets now implement a gridsearch method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3e52523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   length_scale   const  log-likelihood   entropy\n",
      "0        0.0806  0.0102   -12146.699246  0.104471\n",
      "1        0.0806  0.2346     -578.836084  0.104471\n",
      "2        0.0806  0.4590     -381.921518  0.104471\n",
      "3        0.0806  0.6834     -331.086888  0.104471\n",
      "4        0.0806  0.9078     -313.653242  0.104471\n"
     ]
    }
   ],
   "source": [
    "#Lets try manually creating the functions:\n",
    "\n",
    "n_lengthscale = 10\n",
    "n_const = 10\n",
    "\n",
    "#The above values control the number of different hyperparaemters we want to test on\n",
    "\n",
    "lengthscale = np.linspace(0.01*m_length,1.99*m_length,n_lengthscale)\n",
    "const = np.linspace(0.01*m_var,1.99*m_var,n_const)\n",
    "\n",
    "resultsdf = pd.DataFrame({'length_scale':[0.0]*(n_lengthscale*n_const),'const':[0.0]*(n_lengthscale*n_const),\"log-likelihood\":[0.0]*(n_lengthscale*n_const),\"entropy\":[0.0]*(n_lengthscale*n_const)})\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "for i in lengthscale:\n",
    "    for j in const:\n",
    "        kernel = j*RBF(i)\n",
    "        gpc = GaussianProcessRegressor(kernel=kernel,optimizer=None).fit(X, Y)\n",
    "        \n",
    "        #y_probas = cross_val_predict(gpc,X_train,y_train,cv=5,method=\"predict_proba\")\n",
    "        #y_scores = y_probas[:,1]\n",
    "        #resultsdf.iloc[iteration]['AUC'] = calc_AUC(gpc,X_train,y_train)\n",
    "        \n",
    "        resultsdf.iloc[iteration]['log-likelihood'] = gpc.log_marginal_likelihood(theta=None, eval_gradient=False, clone_kernel=True)\n",
    "        \n",
    "        resultsdf.iloc[iteration]['length_scale'] = i\n",
    "        resultsdf.iloc[iteration]['const'] = j\n",
    "        \n",
    "        \n",
    "        resultsdf.iloc[iteration]['entropy'] = calc_entropy(gpc,X_test,y_test)\n",
    "        \n",
    "\n",
    "        #r = permutation_importance(gpc,X_test,y_test,n_repeats=30,random_state=0)\n",
    "        #temp = abs(r.importances_mean)\n",
    "        #tempnew = temp/sum(temp)\n",
    "        #vector1 = np.array(tempnew)\n",
    "        #vector2 = np.array(log_calc(abs(r.importances_mean)))\n",
    "        #resultsdf.iloc[iteration]['entropy'] = -1*np.dot(vector1,vector2)\n",
    "        \n",
    "        iteration+=1\n",
    "\n",
    "print(resultsdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db0abaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsdf = resultsdf.sort_values(by=['entropy'])\n",
    "resultsdf = resultsdf.dropna() #Drop NaNs from too small variance models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cc61202",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Consider dropping innacurate models. We do not care about these models and they could potentially alter the clustering '''\n",
    "\n",
    "max_accuracy = resultsdf['log-likelihood'].max()\n",
    "resultsdf = resultsdf.loc[resultsdf['log-likelihood'] > 1.2*max_accuracy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05a1cd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   length_scale   const  log-likelihood   entropy\n",
      "3        0.0806  0.6834     -331.086888  0.104471\n",
      "4        0.0806  0.9078     -313.653242  0.104471\n",
      "5        0.0806  1.1322     -308.080666  0.104471\n",
      "6        0.0806  1.3566     -307.651925  0.104471\n",
      "7        0.0806  1.5810     -309.703521  0.104471\n",
      "8        0.0806  1.8054     -313.015257  0.104471\n",
      "9        0.0806  2.0298     -316.972413  0.104471\n"
     ]
    }
   ],
   "source": [
    "print(resultsdf.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f66cc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.104471\n",
       "4    0.104471\n",
       "5    0.104471\n",
       "6    0.104471\n",
       "7    0.104471\n",
       "8    0.104471\n",
       "9    0.104471\n",
       "Name: entropy, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsdf['entropy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17fd48",
   "metadata": {},
   "source": [
    "### Lets visualise the accuracy vs interpretability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a8c63e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt = resultsdf.plot.scatter(x=\"entropy\",y=\"log-likelihood\")\n",
    "plt.scatter(resultsdf['entropy'],resultsdf['log-likelihood'])\n",
    "plt.xlabel(\"Feature entropy\")\n",
    "plt.ylabel(\"log-likelihood\")\n",
    "#plt.show()\n",
    "plt.savefig('clustergraph.png',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('clustergraph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6620d052",
   "metadata": {},
   "source": [
    "### We shall now apply K-means to identify the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b309e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many clusters?1\n"
     ]
    }
   ],
   "source": [
    "n = input(\"How many clusters?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99b110ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:881: UserWarning:KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_scale</th>\n",
       "      <th>const</th>\n",
       "      <th>log-likelihood</th>\n",
       "      <th>entropy</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0806</td>\n",
       "      <td>0.6834</td>\n",
       "      <td>-331.086888</td>\n",
       "      <td>0.104471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0806</td>\n",
       "      <td>0.9078</td>\n",
       "      <td>-313.653242</td>\n",
       "      <td>0.104471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0806</td>\n",
       "      <td>1.1322</td>\n",
       "      <td>-308.080666</td>\n",
       "      <td>0.104471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0806</td>\n",
       "      <td>1.3566</td>\n",
       "      <td>-307.651925</td>\n",
       "      <td>0.104471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0806</td>\n",
       "      <td>1.5810</td>\n",
       "      <td>-309.703521</td>\n",
       "      <td>0.104471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length_scale   const  log-likelihood   entropy  cluster\n",
       "3        0.0806  0.6834     -331.086888  0.104471        0\n",
       "4        0.0806  0.9078     -313.653242  0.104471        0\n",
       "5        0.0806  1.1322     -308.080666  0.104471        0\n",
       "6        0.0806  1.3566     -307.651925  0.104471        0\n",
       "7        0.0806  1.5810     -309.703521  0.104471        0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=int(n))\n",
    "c_predicted = km.fit_predict(resultsdf[[\"log-likelihood\",\"entropy\"]])\n",
    "resultsdf[\"cluster\"]=c_predicted\n",
    "resultsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc95e4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#First create a dictionary for colours:\n",
    "\n",
    "colour_dict = {\n",
    "  0: \"blue\",\n",
    "  1: \"red\",\n",
    "  2: \"green\",\n",
    "  3: \"cyan\",\n",
    "  4: \"magenta\",\n",
    "  5: \"yellow\",\n",
    "  6: \"black\",\n",
    "}\n",
    "\n",
    "data_frames = []\n",
    "\n",
    "for i in range(int(n)):\n",
    "    data_frames.append(resultsdf[resultsdf.cluster == i])\n",
    "    plt.scatter(data_frames[i].entropy,data_frames[i][\"log-likelihood\"],color=colour_dict[i])\n",
    "\n",
    "#df1 = resultsdf[resultsdf.cluster == 0]\n",
    "#df2 = resultsdf[resultsdf.cluster == 1]\n",
    "#df3 = resultsdf[resultsdf.cluster == 2]\n",
    "#df4 = resultsdf[resultsdf.cluster == 3]\n",
    "\n",
    "#plt.scatter(df1.entropy,df1[\"log-likelihood\"],color=\"blue\")\n",
    "#plt.scatter(df2.entropy,df2[\"log-likelihood\"],color=\"red\")\n",
    "#plt.scatter(df3.entropy,df3[\"log-likelihood\"],color=\"green\")\n",
    "#plt.scatter(df4.entropy,df4[\"log-likelihood\"],color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ea079",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' It is possible that the user will not be happy with how the data has been assigned.\n",
    "Offer an option for them to redo the clustering . '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af35196a",
   "metadata": {},
   "source": [
    "### We now need to select the median  model (based on entropy) from each of these clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f780ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a list of models and their accuracies:\n",
    "\n",
    "models = {}\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "#Can't use median function because it finds an average for even sets\n",
    "\n",
    "for i in range(int(n)):\n",
    "    index = int(data_frames[i].shape[0]/2)\n",
    "    observation = data_frames[i].iloc[index]\n",
    "    #kernel = float(observation['const'])*RBF(float(observation['length_scale']))\n",
    "    #gp = GaussianProcessClassifier(kernel=kernel,optimizer=None).fit(X, Y)\n",
    "    models['model{}'.format(i)] = [observation['const'],observation['length_scale']]\n",
    "    accuracies.append(observation['log-likelihood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can pull out the parameters we need\n",
    "\n",
    "print(models.get(\"model1\")[0])\n",
    "print(models.get(\"model1\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90d51fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be our PIGEBaQ algorithm:\n",
    "\n",
    "import math\n",
    "\n",
    "def compute_zn(index,data,output_var,np_lengthscale,np_cov,np_mu):\n",
    "    x = np.array(data.iloc[[index]])\n",
    "\n",
    "    element1 = (output_var*math.sqrt(np.linalg.det(np_lengthscale)))/(math.sqrt(np.linalg.det(np_lengthscale+np_cov)))\n",
    "    element2 = np.linalg.inv(np_lengthscale+np_cov)\n",
    "    element3 = np.transpose(np_mu - x)\n",
    "    element4 = -0.5*np.matmul(np.transpose(-1*element3),np.matmul(element2,-1*element3))\n",
    "\n",
    "    zn = element1*math.exp(element4.item())*np.matmul(element2,element3)\n",
    "    return zn\n",
    "\n",
    "def compute_cov(x,y,var,l):  #x and y are each full observations (vectors)\n",
    "    d = 0\n",
    "    m = len(x)\n",
    "    for i in range(m):\n",
    "        d += (x[i]-y[i])**2\n",
    "    val = var*np.exp(-0.5*d/(l**2))\n",
    "    return val\n",
    "\n",
    "\n",
    "def PIGEBaQ(data,var,length):\n",
    "    \n",
    "    this_df = data.dropna()\n",
    "    this_df.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    mu = list(this_df.mean(axis='index'))\n",
    "    del mu[-1]\n",
    "    m = len(mu)\n",
    "    n = len(this_df.index)\n",
    "    np_mu = np.array(mu)\n",
    "    \n",
    "    this_df2 = this_df.drop(columns=['Output'])\n",
    "    df_cov = this_df2.cov()\n",
    "    np_cov = df_cov.to_numpy()\n",
    "    \n",
    "    np_lengthscale = length*np.identity(m)\n",
    "    \n",
    "    Z = [0]*n\n",
    "    for i in range(0,n):\n",
    "        Z[i] = compute_zn(i,this_df2,var,np_lengthscale,np_cov,np_mu)\n",
    "    \n",
    "    Z = np.array(Z)\n",
    "    Z = np.transpose(Z)\n",
    "    Z = Z.reshape(m,n)\n",
    "    \n",
    "    X = this_df2.to_numpy() #Each row is an abservation, each column is a variable\n",
    "    \n",
    "    num = this_df2.shape[0]\n",
    "    K = np.empty([num,num])\n",
    "\n",
    "    #Now lets start filling this matrix:\n",
    "    \n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            K[i,j] = compute_cov(X[i,:],X[j,:],var,length)\n",
    "    \n",
    "    f = this_df[\"Output\"].to_numpy()\n",
    "    f = f.reshape(n)\n",
    "    \n",
    "    E = np.matmul(Z,np.matmul(np.linalg.inv(K),f))\n",
    "    \n",
    "    output_df = pd.DataFrame({\"Skills\":this_df2.columns,\"Importance\":E})\n",
    "    output_df = output_df.sort_values(by=\"Importance\")\n",
    "    output_df.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "    output_df['Importance'] = output_df['Importance']/math.sqrt(output_df['Importance'].pow(2).sum()) #Normalizes values\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7e7f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data_PIG = scaled_total_set.drop(columns=['O*NET-SOC Code', 'Title'])\n",
    "input_data_PIG = scaled_total_set.copy()\n",
    "\n",
    "tables = []\n",
    "\n",
    "for i in range(int(n)):\n",
    "    tables.append(PIGEBaQ(input_data_PIG,models.get(\"model{}\".format(i))[0],models.get(\"model{}\".format(i))[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9dddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bcef37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " <ipython-input-31-2c5ce980b0df>:13: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.913119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.379835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.067645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.131769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Features  Importance\n",
       "0        1   -0.913119\n",
       "1        2   -0.379835\n",
       "2        3   -0.067645\n",
       "3        0    0.131769"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We shall now generate a single dataframe which has the weighted explanations:\n",
    "\n",
    "explanations_df = pd.DataFrame({\"Features\":np.array(scaled_total_set.columns[0:4])})\n",
    "explanations_df[\"Importance\"] = 0.0\n",
    "#explanations_df\n",
    "\n",
    "model = tables[0] #This is the relevant importance dataframe\n",
    "\n",
    "#explanations_df[\"Importance\"][0] = (accuracies[0]/sum(accuracies))*model['Importance'][model.index[model['Skills'] == explanations_df['Features'][0]]]\n",
    "for j in range(int(n)):\n",
    "    model = tables[j]\n",
    "    for i in range(int(explanations_df.shape[0])):\n",
    "        explanations_df[\"Importance\"][i] += (accuracies[j]/sum(accuracies))*model['Importance'][model.index[model['Skills'] == explanations_df['Features'][i]]]\n",
    "\n",
    "\n",
    "explanations_df = explanations_df.sort_values(by=[\"Importance\"],ascending=True)\n",
    "explanations_df.reset_index(drop = True, inplace = True)\n",
    "explanations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4019a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations_df.to_csv(\"C:/Users/jacob/Documents/4YP data/DataSynthesis/Regression/4x200/interpretation.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbca202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explanations_df.to_csv(\"perm_PIGEBaQ.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the moment, we shall generate an explanation based on feature importance across the different clusters:\n",
    "\n",
    "\n",
    "tables = []\n",
    "\n",
    "for i in range(int(n)):\n",
    "    tables.append(pd.DataFrame({\"Features\":np.array(training_set.columns[2:37])}))\n",
    "\n",
    "\n",
    "for i in range(int(n)):\n",
    "    r = permutation_importance(models[i],X_test,y_test,n_repeats=30,random_state=0)\n",
    "    #tables[i][\"Importances\"] = abs(r.importances_mean)\n",
    "    tables[i][\"Importances\"] = r.importances_mean\n",
    "    tables[i] = tables[i].sort_values(by=[\"Importances\"],ascending=False)\n",
    "    tables[i].reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "#for i in range(int(n)):\n",
    "    #r = permutation_importance(models[i],X_test,y_test,n_repeats=30,random_state=0)\n",
    "    #importance_table[\"Importances{}\".format(i)] = abs(r.importances_mean)\n",
    "    #importance_table = importance_table.sort_values(by=['Importances{}'.format(i)])\n",
    "\n",
    "#NOTE - I'm uneasy about using the same test sets as before, should I expand to the unknown jobs?\n",
    "\n",
    "importance_dict = {}\n",
    "for i in range(int(n)):\n",
    "    q = tables[i]\n",
    "    importance_dict[\"Importance{}\".format(i)] = q[\"Features\"]\n",
    "#q = tables[1]\n",
    "#importance_dict[\"Importance1\"] = q[\"Features\"]\n",
    "importance_table = pd.DataFrame(data=importance_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d12e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importance_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec713c4",
   "metadata": {},
   "source": [
    "### We now have a table of features sorted by how important they are for each cluster, based on the permutation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4994965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shall generate one more table, which will have a row for each feature and a score for how important it is:\n",
    "\n",
    "final_importance_table = pd.DataFrame({\"Features\":np.array(training_set.columns[2:37]),\"Importance\":np.zeros(np.shape(training_set.columns[2]))})\n",
    "feature_list = final_importance_table['Features'].tolist()\n",
    "#final_importance_table.head()\n",
    "for i in feature_list:\n",
    "    for j in range(int(n)):\n",
    "        index_val = importance_table.index[importance_table[\"Importance{}\".format(j)]==i]\n",
    "        index_val = index_val*accuracies[j]/sum(accuracies)\n",
    "        final_importance_table.loc[final_importance_table[\"Features\"]==i,\"Importance\"] += index_val.tolist()[0]\n",
    "    \n",
    "final_importance_table = final_importance_table.sort_values(by=[\"Importance\"],ascending=True)\n",
    "final_importance_table.reset_index(drop = True, inplace = True)\n",
    "final_importance_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e26c97",
   "metadata": {},
   "source": [
    "### We now have a table which ranks features by importance across all the clusters! Note that is values each cluster equally - this might be something we can improve upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1470c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets divert our attention towards making predictions on our unknown jobs:\n",
    "\n",
    "unknown_jobs = df4[df4.isna().any(axis=1)] #This is our set of unknown jobs\n",
    "#SCALE THE DATA! - Do I need to fit a new scaler?\n",
    "X = scaler.transform(unknown_jobs.iloc[:,2:37])\n",
    "#X = np.array(unknown_jobs.iloc[:,2:37])\n",
    "print(X)\n",
    "#for i in int(n):\n",
    "    #models[i].predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = gpc.predict_proba(X)\n",
    "test1 = gpc.predict(X)\n",
    "test2 = gpc.predict_proba(X)\n",
    "print(test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now fill in the dataframe:\n",
    "\n",
    "unknown_jobs[\"Auto label value\"] = test1\n",
    "unknown_jobs[\"Auto probability\"] = test2[:,1]\n",
    "unknown_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b143b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_jobs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets think about how we can apply the model from each of our clusters and compute an average:\n",
    "\n",
    "for i in range(int(n)):\n",
    "    probs = models[i].predict_proba(X)\n",
    "    unknown_jobs[\"Auto probability{}\".format(i)] = probs[:,1]\n",
    "\n",
    "#We need a list of column titles:\n",
    "column_titles = []\n",
    "for i in range(int(n)):\n",
    "    column_titles.append(\"Auto probability{}\".format(i))\n",
    "\n",
    "#Now calculate mean automotability    \n",
    "\n",
    "unknown_jobs[\"Auto probability\"] = unknown_jobs[column_titles].mean(axis=1)\n",
    "\n",
    "#And finally apply a function to determine auto-label value:\n",
    "\n",
    "def label_auto(my_input):\n",
    "    if my_input - 0.5 < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "unknown_jobs[\"Auto label value\"] = unknown_jobs[\"Auto probability\"].apply(label_auto)\n",
    "\n",
    "unknown_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d836b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unknown_jobs[\"Title\"][\"i\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475056f",
   "metadata": {},
   "source": [
    "### We have now generated values for automotability!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
