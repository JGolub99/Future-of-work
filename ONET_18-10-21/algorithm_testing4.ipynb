{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e133cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This version reframes the problem as a regression case'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Skills.xlsx\")\n",
    "#print(df.head())\n",
    "\n",
    "#Lets remove the importance values:\n",
    "\n",
    "df2 = df.loc[df[\"Scale Name\"] == \"Level\"]\n",
    "df2.reset_index(drop = True, inplace = True)\n",
    "#print(df2.head())\n",
    "\n",
    "#Lets now remove the irrelevent columns:\n",
    "\n",
    "df3 = df2.drop(columns = [\"Scale ID\",\"Scale Name\",\"N\",\"Recommend Suppress\",\"Not Relevant\",\"Date\",\"Domain Source\"])\n",
    "print(df3.head())\n",
    "\n",
    "#NOTE that we have ignored the suppress recomendations, we shall continue with this for now but will need to address this later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420df498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now discover a bit about our data set:\n",
    "\n",
    "df3.info()\n",
    "\n",
    "#We note that there are some occupations for which the standard error and bound values are missing. Lets supress these for now:\n",
    "\n",
    "df3.drop(columns = [\"Standard Error\",\"Lower CI Bound\",\"Upper CI Bound\"],inplace = True)\n",
    "print(df3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a1537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I shall implement the drop_duplicates method:\n",
    "\n",
    "df4 = df3[[\"O*NET-SOC Code\",\"Title\"]]\n",
    "df4.drop_duplicates(inplace=True)\n",
    "df4.reset_index(drop = True, inplace = True)\n",
    "#print(df4.head())\n",
    "\n",
    "#We now need to add the variables. Begin by adding empty columns to the dataframe:\n",
    "\n",
    "n_jobs = len(set((df3[\"Title\"])))\n",
    "n_variables = len(set((df3[\"Element Name\"])))\n",
    "\n",
    "for i in range(n_jobs):\n",
    "    df4[df3[\"Element Name\"][i]] = \"\"\n",
    "\n",
    "#print(df4.head())\n",
    "#Now we need to fill these columns:\n",
    "\n",
    "x = df3.loc[df3[\"Title\"] == \"Chief Executives\"]\n",
    "y = x[\"Data Value\"]\n",
    "\n",
    "for i in range(n_variables):\n",
    "    df4[df4.columns[2+i]][0] = y[i]\n",
    "    \n",
    "#We now need to do this procedure for every job:\n",
    "\n",
    "for j in range(n_jobs):\n",
    "    x = df3.loc[df3[\"Title\"] == df4.iloc[j,1]]\n",
    "    y = x[\"Data Value\"]\n",
    "    y.reset_index(drop = True, inplace = True)\n",
    "    for i in range(n_variables):\n",
    "        df4[df4.columns[2+i]][j] = y[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0748248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df4.head())\n",
    "print(df4.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194be949",
   "metadata": {},
   "source": [
    "### We now have the skills dataframe just as we want it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edae9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets understand our data a bit:\n",
    "\n",
    "df4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "hist = df4.iloc[:,20].hist(bins=20)\n",
    "\n",
    "#We can inspect the histogram of any variable we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628f6e5",
   "metadata": {},
   "source": [
    "### Let's now  import another dataframe with autovalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87641e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_excel(\"US_data_email.xls\")\n",
    "df5 = df5[[\"Occupation Name\",\"BLS codes\",\"Training set automatable labels\"]]\n",
    "print(df5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40868f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422495f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets define a function so that we can make occupation ID's consistent:\n",
    "\n",
    "def title_set(my_string):\n",
    "    my_list = []\n",
    "    my_list[:0] = my_string\n",
    "    my_list.remove(\"_\")\n",
    "    my_list.append(\".00\")\n",
    "    my_output = \"\".join(my_list)\n",
    "    return my_output\n",
    "\n",
    "#print(title_set(\"45-4023_\"))\n",
    "\n",
    "df5.iloc[:,1] = df5.iloc[:,1].apply(title_set)\n",
    "print(df5.head())\n",
    "df5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88670d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now concatenate the auto labels to our 1st dataframe:\n",
    "import numpy as np\n",
    "df4[\"Auto label value\"] = np.nan\n",
    "for i in list(df5[\"BLS codes\"]):\n",
    "    df4.loc[df4[\"O*NET-SOC Code\"] == i,\"Auto label value\"] = list(df5.loc[df5[\"BLS codes\"] == i,\"Training set automatable labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.info()\n",
    "\n",
    "#Note - the auto value count is supposedly 330 non-null, even though it should be 70. After creating a csv from the dataframe,\n",
    "#I found that there were 70 non-null as expected. Worth bringing up with Mike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035d0d9",
   "metadata": {},
   "source": [
    "### START FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec8cea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.937828</td>\n",
       "      <td>2.146718</td>\n",
       "      <td>1.402961</td>\n",
       "      <td>2.618168</td>\n",
       "      <td>0.502278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.121296</td>\n",
       "      <td>2.447643</td>\n",
       "      <td>4.396986</td>\n",
       "      <td>2.033471</td>\n",
       "      <td>-0.011970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.940303</td>\n",
       "      <td>4.387353</td>\n",
       "      <td>3.075637</td>\n",
       "      <td>4.275894</td>\n",
       "      <td>0.313661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.071174</td>\n",
       "      <td>3.300124</td>\n",
       "      <td>3.449231</td>\n",
       "      <td>1.474823</td>\n",
       "      <td>-0.374145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.358594</td>\n",
       "      <td>2.397611</td>\n",
       "      <td>1.082828</td>\n",
       "      <td>2.004212</td>\n",
       "      <td>1.151621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3    Output\n",
       "0  2.937828  2.146718  1.402961  2.618168  0.502278\n",
       "1  4.121296  2.447643  4.396986  2.033471 -0.011970\n",
       "2  3.940303  4.387353  3.075637  4.275894  0.313661\n",
       "3  4.071174  3.300124  3.449231  1.474823 -0.374145\n",
       "4  1.358594  2.397611  1.082828  2.004212  1.151621"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df4 = pd.read_csv(\"C:/Users/jacob/Documents/4YP data/DataSynthesis/Regression/4x400/data.csv\")\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2d9a8",
   "metadata": {},
   "source": [
    "### We now have a dataset which encompass jobs titles, SOC codes, skill levels and hand picked auto labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d2e87",
   "metadata": {},
   "source": [
    "### Lets now split and standardize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To prevent information about the distribution of the test set leaking into the model, we shall first form a training set\n",
    "# and form a scaler operator from this, and then apply this to both training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fc4bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       400 non-null    float64\n",
      " 1   1       400 non-null    float64\n",
      " 2   2       400 non-null    float64\n",
      " 3   3       400 non-null    float64\n",
      " 4   Output  400 non-null    float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 15.8 KB\n"
     ]
    }
   ],
   "source": [
    "#Lets now create a training set which includes only the jobs for which we have hand picked auto values:\n",
    "\n",
    "training_set = df4.dropna(axis=0,how=\"any\")\n",
    "training_set.reset_index(drop = True, inplace=True)\n",
    "#print(training_set.head())\n",
    "training_set.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c955b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets apply stratified sampling on this set to create a training and test set\n",
    "#Code taken from Hands on Machine Learning book\n",
    "\n",
    "strat_train_set = df4.head(int(400*0.8))\n",
    "strat_test_set = df4.tail(int(400*0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04871038",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set.reset_index(drop=True,inplace=True)\n",
    "strat_test_set.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4985df74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Now that we have our training set, lets create a standardiser for it:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=True,with_std=True)\n",
    "scaler.fit(strat_train_set.iloc[:,0:4])\n",
    "scaled_training_values = scaler.transform(strat_train_set.iloc[:,0:4])\n",
    "scaled_test_values = scaler.transform(strat_test_set.iloc[:,0:4])\n",
    "scaled_train_set = strat_train_set.copy()\n",
    "scaled_test_set = strat_test_set.copy()\n",
    "#print(strat_train_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85fb4ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary = pd.DataFrame(data=scaled_training_values)\n",
    "temporary2 = pd.DataFrame(data=scaled_test_values) #we create temporary data frames from the numpy arrays we've just created\n",
    "#print(temporary)\n",
    "for i in range(0,4):\n",
    "    scaled_train_set[scaled_train_set.columns[i]] = temporary[temporary.columns[i]]\n",
    "    scaled_test_set[scaled_test_set.columns[i]] = temporary2[temporary2.columns[i]]\n",
    "\n",
    "#print(scaled_test_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0efe1133",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' We shall also create a standardized set for ALL of the training data '''\n",
    "\n",
    "scaler2 = StandardScaler(with_mean=True,with_std=True)\n",
    "scaler2.fit(strat_train_set.iloc[:,0:4])\n",
    "scaled_total_values = scaler2.transform(training_set.iloc[:,0:4])\n",
    "scaled_total_set = training_set.copy()\n",
    "\n",
    "temporary3 = pd.DataFrame(data=scaled_total_values)\n",
    "for i in range(0,4):\n",
    "    scaled_total_set[scaled_total_set.columns[i]] = temporary3[temporary3.columns[i]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d735dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3    Output\n",
      "0  0.020180 -0.780196 -1.359871 -0.288062  0.502278\n",
      "1  1.049649 -0.506783  1.228460 -0.787441 -0.011970\n",
      "2  0.892208  1.255587  0.086156  1.127770  0.313661\n",
      "3  1.006049  0.267759  0.409127 -1.264571 -0.374145\n",
      "4 -1.353554 -0.552241 -1.636626 -0.812431  1.151621\n",
      "          0         1         2         3    Output\n",
      "0 -1.339511  0.997431  0.417787  0.833368  0.920610\n",
      "1 -1.270359 -0.546316  1.126875 -1.569635  0.561259\n",
      "2  0.384480 -0.073693 -0.034206 -0.383413  0.256888\n",
      "3 -1.054845  1.129135  0.706726 -0.585152  2.250965\n",
      "4  1.062953 -1.676313  1.367267  1.017761 -1.247006\n",
      "          0         1         2         3    Output\n",
      "0  0.020180 -0.780196 -1.359871 -0.288062  0.502278\n",
      "1  1.049649 -0.506783  1.228460 -0.787441 -0.011970\n",
      "2  0.892208  1.255587  0.086156  1.127770  0.313661\n",
      "3  1.006049  0.267759  0.409127 -1.264571 -0.374145\n",
      "4 -1.353554 -0.552241 -1.636626 -0.812431  1.151621\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train_set.head())\n",
    "print(scaled_test_set.head())\n",
    "print(scaled_total_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67dc1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_total_set.to_csv(\"PIGEBAQ_testset.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5c7ed",
   "metadata": {},
   "source": [
    "### We now have a fully scaled training and test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78aad85",
   "metadata": {},
   "source": [
    "### We can now perform Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfe4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We begin by creating a centred training set:\n",
    "\n",
    "X = strat_train_set.drop([\"Title\",\"O*NET-SOC Code\"],axis=1)\n",
    "\n",
    "#We now use the Scikit learn toolkit to visualise how the explained variance ratio changes with no. dimensions:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "dim = range(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(dim),np.array(cumsum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Having visualized the effect of dimensionality, we can implement this to our dataset:\n",
    "\n",
    "pca2 = PCA(n_components=0.95)\n",
    "X_reduced = pca2.fit_transform(X)\n",
    "print(X_reduced[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badabc02",
   "metadata": {},
   "source": [
    "### We shall now fit a GP classifier to the unreduced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b04605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844296b79ab9437a922c49ab508410d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(IntProgress(value=0, max=1000), HTML(value=''))), Box(children=(HTML(value=''),)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<paramz.optimization.optimization.opt_lbfgsb at 0x1980b017520>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Begin by creating numpy arrays for our input X and output Y:\n",
    "\n",
    "#X = np.array([scaled_train_set.iloc[:,0:4]])\n",
    "#Y = np.array([scaled_train_set.iloc[:,4]])\n",
    "X = np.array([scaled_total_set.iloc[:,0:4]])\n",
    "Y = np.array([scaled_total_set.iloc[:,4]])\n",
    "#X = np.transpose(X)\n",
    "#Y = np.transpose(Y)\n",
    "\n",
    "\n",
    "X = np.reshape(X,(400,4)) #Reshape to go from 3d matrix to 2d\n",
    "Y = np.reshape(Y,(400,1)) # ^\n",
    "\n",
    "#Now generate a kernel:\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "import GPy\n",
    "\n",
    "kernel = GPy.kern.RBF(input_dim=4, variance=100., lengthscale=100.)\n",
    "m_gpy = GPy.models.GPRegression(X,Y,kernel)\n",
    "m_gpy.optimize(messages=True)\n",
    "#m_gpy.optimize_restarts(num_restarts = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5691efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the rbf.variance?0.66\n",
      "What is the rbf.lengthscale?1.09\n"
     ]
    }
   ],
   "source": [
    "#We shall request values for the variance and lengthscale:\n",
    "\n",
    "m_var = input(\"What is the rbf.variance?\")\n",
    "m_length = input(\"What is the rbf.lengthscale?\")\n",
    "\n",
    "m_var = float(m_var)\n",
    "m_length = float(m_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e436ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate this model in scikit learn\n",
    "\n",
    "\n",
    "sci_kernel = m_var * RBF(m_length)\n",
    "gpc = GaussianProcessRegressor(kernel=sci_kernel,optimizer=None).fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de51ce",
   "metadata": {},
   "source": [
    "### Lets now apply k-fold cross validation on the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56997c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(scaled_train_set.iloc[:,0:4])\n",
    "#X_train = np.transpose(X_train)\n",
    "y_train = np.array(scaled_train_set.iloc[:,4])\n",
    "#y_train = np.transpose(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6974f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train,y_train):\n",
    "    clone_gpc = clone(gpc)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train[test_index]\n",
    "    \n",
    "    clone_gpc.fit(X_train_folds,y_train_folds)\n",
    "    y_pred = clone_gpc.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct/len(y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68786926",
   "metadata": {},
   "source": [
    "### What about an F1 score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185668f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_train_pred = cross_val_predict(gpc,X_train,y_train,cv=5)\n",
    "f1_score(y_train,y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c6f32",
   "metadata": {},
   "source": [
    "### And how about an AUC value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40910d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calc_AUC(gpc,X_train,y_train):\n",
    "    y_probas = cross_val_predict(gpc,X_train,y_train,cv=5,method=\"predict_proba\")\n",
    "    y_scores = y_probas[:,1]\n",
    "    return roc_auc_score(y_train,y_scores)\n",
    "\n",
    "calc_AUC(gpc,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ddfdf",
   "metadata": {},
   "source": [
    "### And a log-likelihood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "424fa99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295.8023204749524"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpc.log_marginal_likelihood(theta=None, eval_gradient=False, clone_kernel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d78e0",
   "metadata": {},
   "source": [
    "### It is worth using the model to predict values for the test set to ensure it is working as I want it to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4d045d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(scaled_test_set.iloc[:,0:4])\n",
    "y_test = np.array(scaled_test_set.iloc[:,4])\n",
    "\n",
    "y_pred = gpc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09f0c78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.92061018  0.56125855  0.25688767  2.25096548 -1.24700556 -0.53950911\n",
      "  1.58947328 -1.0386724   2.64875169 -0.47384822  0.17692174  0.70934445\n",
      " -0.06361754  0.04252402  0.47906104  0.20331176 -0.01038017  2.01120186\n",
      "  0.19673988 -0.93078223 -0.22607017  1.14364672  0.55759703  0.24492461\n",
      " -0.10751673  1.56373083 -0.69677177 -0.51697352 -0.81103336  0.9775855\n",
      "  0.20879102 -0.20577722 -0.16198346 -0.28427011  0.58785939  1.01449062\n",
      "  0.28512449 -0.15259548  0.70326325  0.4753244   1.02083802 -0.37581995\n",
      " -1.01270665 -0.27021513  0.37353949 -0.92051808 -0.52785489 -0.74425849\n",
      " -0.81805523 -0.30148904  0.03378311  0.78212255  1.53949342  1.4113644\n",
      " -0.19050906  0.798755    1.16772901  0.2718694  -0.30734303 -0.11907326\n",
      " -0.18476192  0.17618988  0.69869541 -0.42927744  1.22062366  1.51423736\n",
      "  1.21744053 -0.56753006  0.58144376  0.29192944  0.85776029 -0.03960504\n",
      "  0.35254347  0.4758597   2.42639532 -1.09828008  0.18828421  0.28434534\n",
      "  0.74557302  0.5295743 ]\n",
      "[[ 0.92061018]\n",
      " [ 0.56125855]\n",
      " [ 0.25688767]\n",
      " [ 2.25096548]\n",
      " [-1.24700556]\n",
      " [-0.53950911]\n",
      " [ 1.58947327]\n",
      " [-1.03867241]\n",
      " [ 2.64875169]\n",
      " [-0.47384822]\n",
      " [ 0.17692174]\n",
      " [ 0.70934445]\n",
      " [-0.06361754]\n",
      " [ 0.04252402]\n",
      " [ 0.47906104]\n",
      " [ 0.20331176]\n",
      " [-0.01038017]\n",
      " [ 2.01120183]\n",
      " [ 0.19673988]\n",
      " [-0.93078223]\n",
      " [-0.22607017]\n",
      " [ 1.14364671]\n",
      " [ 0.55759703]\n",
      " [ 0.24492461]\n",
      " [-0.10751673]\n",
      " [ 1.56373083]\n",
      " [-0.69677176]\n",
      " [-0.51697353]\n",
      " [-0.81103336]\n",
      " [ 0.9775855 ]\n",
      " [ 0.20879103]\n",
      " [-0.20577722]\n",
      " [-0.16198344]\n",
      " [-0.28427011]\n",
      " [ 0.58785939]\n",
      " [ 1.01449062]\n",
      " [ 0.28512449]\n",
      " [-0.15259548]\n",
      " [ 0.70326325]\n",
      " [ 0.47532441]\n",
      " [ 1.02083802]\n",
      " [-0.37581995]\n",
      " [-1.01270665]\n",
      " [-0.27021513]\n",
      " [ 0.3735395 ]\n",
      " [-0.92051808]\n",
      " [-0.52785489]\n",
      " [-0.74425849]\n",
      " [-0.81805523]\n",
      " [-0.30148904]\n",
      " [ 0.03378311]\n",
      " [ 0.78212255]\n",
      " [ 1.53949342]\n",
      " [ 1.41136439]\n",
      " [-0.19050906]\n",
      " [ 0.79875499]\n",
      " [ 1.16772902]\n",
      " [ 0.2718694 ]\n",
      " [-0.30734303]\n",
      " [-0.11907326]\n",
      " [-0.18476193]\n",
      " [ 0.17618988]\n",
      " [ 0.69869541]\n",
      " [-0.42927744]\n",
      " [ 1.22062366]\n",
      " [ 1.51423735]\n",
      " [ 1.21744054]\n",
      " [-0.56753006]\n",
      " [ 0.58144376]\n",
      " [ 0.29192944]\n",
      " [ 0.85776028]\n",
      " [-0.03960504]\n",
      " [ 0.35254348]\n",
      " [ 0.47585969]\n",
      " [ 2.42639532]\n",
      " [-1.09828008]\n",
      " [ 0.18828421]\n",
      " [ 0.28434534]\n",
      " [ 0.74557302]\n",
      " [ 0.52957431]]\n",
      "[[ 2.23064467e-09 -3.59351630e-01 -6.63722508e-01 ... -6.36264842e-01\n",
      "  -1.75037163e-01 -3.91035877e-01]\n",
      " [ 3.59351630e-01 -1.39022205e-09 -3.04370880e-01 ... -2.76913214e-01\n",
      "   1.84314465e-01 -3.16842487e-02]\n",
      " [ 6.63722509e-01  3.04370877e-01 -1.49554652e-09 ...  2.74576645e-02\n",
      "   4.88685343e-01  2.72686629e-01]\n",
      " ...\n",
      " [ 6.36264842e-01  2.76913210e-01 -2.74576681e-02 ... -2.13717771e-09\n",
      "   4.61227676e-01  2.45228963e-01]\n",
      " [ 1.75037164e-01 -1.84314468e-01 -4.88685346e-01 ... -4.61227680e-01\n",
      "  -1.54432833e-09 -2.15998715e-01]\n",
      " [ 3.91035874e-01  3.16842419e-02 -2.72686636e-01 ... -2.45228970e-01\n",
      "   2.15998708e-01 -5.48039425e-09]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(y_pred)\n",
    "print(y_test-y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd5b80",
   "metadata": {},
   "source": [
    "### We now have a fully working GP classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52436a7e",
   "metadata": {},
   "source": [
    "### Lets now consider the interpretability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c840583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Features  Importances\n",
      "2        2     0.400707\n",
      "3        3     0.425863\n",
      "1        1     0.790145\n",
      "0        0     1.422759\n"
     ]
    }
   ],
   "source": [
    "#We shall use the feature permutation method:\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "feature_importance = pd.DataFrame({\"Features\":np.array(training_set.columns[0:4])})\n",
    "\n",
    "r = permutation_importance(gpc,X_test,y_test,n_repeats=30,random_state=0)\n",
    "feature_importance[\"Importances\"] = abs(r.importances_mean)\n",
    "feature_importance = feature_importance.sort_values(by=['Importances'])\n",
    "#feature_importance = feature_importance\n",
    "print(feature_importance)\n",
    "#print(r.importances_mean)\n",
    "\n",
    "#for i in r.importances_mean.argsort()[::-1]:\n",
    "#    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "#        print(f\"{feature_importance.Features[i]:<8}\"\n",
    "#        f\"{r.importances_mean[i]:.3f}\"\n",
    "#        f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb49c78",
   "metadata": {},
   "source": [
    "### Now that we have the feature importances, lets calculate the entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79ce2fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.136351878209846\n"
     ]
    }
   ],
   "source": [
    "# For the moment we won't normalise the distribution - might have to do this in the future (ask Mike)\n",
    "\n",
    "def log_calc(my_list):    #This function deals with values of 0\n",
    "    my_output = [0]*len(my_list)\n",
    "    for i in range(len(my_list)):\n",
    "        if my_list[i] != 0.0:\n",
    "            my_output[i] = np.log(my_list[i])\n",
    "    return my_output        \n",
    "            \n",
    "temp = abs(r.importances_mean)\n",
    "tempnew = temp/sum(temp)\n",
    "vector1 = np.array(tempnew)\n",
    "vector2 = np.array(log_calc(abs(r.importances_mean)))\n",
    "entropy = -1*np.dot(vector1,vector2)\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16486e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.136351878209846"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets create a function that does all of this:\n",
    "\n",
    "def calc_entropy(gpc,X_test,y_test):\n",
    "    r = permutation_importance(gpc,X_test,y_test,n_repeats=30,random_state=0)\n",
    "    temp = abs(r.importances_mean)\n",
    "    tempnew = temp/sum(temp)\n",
    "    vector1 = np.array(tempnew)\n",
    "    vector2 = np.array(log_calc(abs(r.importances_mean)))\n",
    "    return -1*np.dot(vector1,vector2)\n",
    "\n",
    "calc_entropy(gpc,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d928b",
   "metadata": {},
   "source": [
    "### Lets now implement a gridsearch method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3e52523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   length_scale   const  log-likelihood  entropy\n",
      "0        0.0109  0.0066   -20223.406881 -0.09057\n",
      "1        0.0109  0.1518     -897.492300 -0.09057\n",
      "2        0.0109  0.2970     -588.325650 -0.09057\n",
      "3        0.0109  0.4422     -515.719631 -0.09057\n",
      "4        0.0109  0.5874     -495.547234 -0.09057\n"
     ]
    }
   ],
   "source": [
    "#Lets try manually creating the functions:\n",
    "\n",
    "n_lengthscale = 10\n",
    "n_const = 10\n",
    "\n",
    "#The above values control the number of different hyperparaemters we want to test on\n",
    "\n",
    "lengthscale = np.linspace(0.01*m_length,1.99*m_length,n_lengthscale)\n",
    "const = np.linspace(0.01*m_var,1.99*m_var,n_const)\n",
    "\n",
    "resultsdf = pd.DataFrame({'length_scale':[0.0]*(n_lengthscale*n_const),'const':[0.0]*(n_lengthscale*n_const),\"log-likelihood\":[0.0]*(n_lengthscale*n_const),\"entropy\":[0.0]*(n_lengthscale*n_const)})\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "for i in lengthscale:\n",
    "    for j in const:\n",
    "        kernel = j*RBF(i)\n",
    "        gpc = GaussianProcessRegressor(kernel=kernel,optimizer=None).fit(X, Y)\n",
    "        \n",
    "        #y_probas = cross_val_predict(gpc,X_train,y_train,cv=5,method=\"predict_proba\")\n",
    "        #y_scores = y_probas[:,1]\n",
    "        #resultsdf.iloc[iteration]['AUC'] = calc_AUC(gpc,X_train,y_train)\n",
    "        \n",
    "        resultsdf.iloc[iteration]['log-likelihood'] = gpc.log_marginal_likelihood(theta=None, eval_gradient=False, clone_kernel=True)\n",
    "        \n",
    "        resultsdf.iloc[iteration]['length_scale'] = i\n",
    "        resultsdf.iloc[iteration]['const'] = j\n",
    "        \n",
    "        \n",
    "        resultsdf.iloc[iteration]['entropy'] = calc_entropy(gpc,X_test,y_test)\n",
    "        \n",
    "\n",
    "        #r = permutation_importance(gpc,X_test,y_test,n_repeats=30,random_state=0)\n",
    "        #temp = abs(r.importances_mean)\n",
    "        #tempnew = temp/sum(temp)\n",
    "        #vector1 = np.array(tempnew)\n",
    "        #vector2 = np.array(log_calc(abs(r.importances_mean)))\n",
    "        #resultsdf.iloc[iteration]['entropy'] = -1*np.dot(vector1,vector2)\n",
    "        \n",
    "        iteration+=1\n",
    "\n",
    "print(resultsdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db0abaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsdf = resultsdf.sort_values(by=['entropy'])\n",
    "resultsdf = resultsdf.dropna() #Drop NaNs from too small variance models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cc61202",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Consider dropping innacurate models. We do not care about these models and they could potentially alter the clustering '''\n",
    "\n",
    "max_accuracy = resultsdf['log-likelihood'].max()\n",
    "resultsdf = resultsdf.loc[resultsdf['log-likelihood'] < 1.2*max_accuracy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05a1cd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    length_scale   const  log-likelihood   entropy\n",
      "0         0.0109  0.0066   -2.022341e+04 -0.090570\n",
      "1         0.0109  0.1518   -8.974923e+02 -0.090570\n",
      "2         0.0109  0.2970   -5.883256e+02 -0.090570\n",
      "3         0.0109  0.4422   -5.157196e+02 -0.090570\n",
      "4         0.0109  0.5874   -4.955472e+02 -0.090570\n",
      "5         0.0109  0.7326   -4.932720e+02 -0.090570\n",
      "6         0.0109  0.8778   -4.983500e+02 -0.090570\n",
      "7         0.0109  1.0230   -5.067038e+02 -0.090570\n",
      "8         0.0109  1.1682   -5.165212e+02 -0.090570\n",
      "9         0.0109  1.3134   -5.269232e+02 -0.090570\n",
      "90        2.1691  0.0066   -5.054075e+07  0.104477\n",
      "80        1.9293  0.0066   -1.359833e+07  0.115151\n",
      "91        2.1691  0.1518   -3.873826e+06  0.116547\n",
      "92        2.1691  0.2970   -2.100663e+06  0.118648\n",
      "93        2.1691  0.4422   -1.447554e+06  0.119627\n",
      "94        2.1691  0.5874   -1.105653e+06  0.120191\n",
      "95        2.1691  0.7326   -8.948270e+05  0.120556\n",
      "96        2.1691  0.8778   -7.516726e+05  0.120810\n",
      "97        2.1691  1.0230   -6.480563e+05  0.120996\n",
      "98        2.1691  1.1682   -5.695607e+05  0.121138\n"
     ]
    }
   ],
   "source": [
    "print(resultsdf.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f66cc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    -0.090570\n",
       "1    -0.090570\n",
       "2    -0.090570\n",
       "3    -0.090570\n",
       "4    -0.090570\n",
       "        ...   \n",
       "24    0.311509\n",
       "23    0.311509\n",
       "22    0.311509\n",
       "21    0.311509\n",
       "20    0.311509\n",
       "Name: entropy, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsdf['entropy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17fd48",
   "metadata": {},
   "source": [
    "### Lets visualise the accuracy vs interpretability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a8c63e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt = resultsdf.plot.scatter(x=\"entropy\",y=\"log-likelihood\")\n",
    "plt.scatter(resultsdf['entropy'],resultsdf['log-likelihood'])\n",
    "plt.xlabel(\"Feature entropy\")\n",
    "plt.ylabel(\"log-likelihood\")\n",
    "#plt.show()\n",
    "plt.savefig('clustergraph.png',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('clustergraph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6620d052",
   "metadata": {},
   "source": [
    "### We shall now apply K-means to identify the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b309e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many clusters?1\n"
     ]
    }
   ],
   "source": [
    "n = input(\"How many clusters?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99b110ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:881: UserWarning:KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_scale</th>\n",
       "      <th>const</th>\n",
       "      <th>log-likelihood</th>\n",
       "      <th>entropy</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>-20223.406881</td>\n",
       "      <td>-0.09057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.1518</td>\n",
       "      <td>-897.492300</td>\n",
       "      <td>-0.09057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>-588.325650</td>\n",
       "      <td>-0.09057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.4422</td>\n",
       "      <td>-515.719631</td>\n",
       "      <td>-0.09057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.5874</td>\n",
       "      <td>-495.547234</td>\n",
       "      <td>-0.09057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length_scale   const  log-likelihood  entropy  cluster\n",
       "0        0.0109  0.0066   -20223.406881 -0.09057        0\n",
       "1        0.0109  0.1518     -897.492300 -0.09057        0\n",
       "2        0.0109  0.2970     -588.325650 -0.09057        0\n",
       "3        0.0109  0.4422     -515.719631 -0.09057        0\n",
       "4        0.0109  0.5874     -495.547234 -0.09057        0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=int(n))\n",
    "c_predicted = km.fit_predict(resultsdf[[\"log-likelihood\",\"entropy\"]])\n",
    "resultsdf[\"cluster\"]=c_predicted\n",
    "resultsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc95e4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#First create a dictionary for colours:\n",
    "\n",
    "colour_dict = {\n",
    "  0: \"blue\",\n",
    "  1: \"red\",\n",
    "  2: \"green\",\n",
    "  3: \"cyan\",\n",
    "  4: \"magenta\",\n",
    "  5: \"yellow\",\n",
    "  6: \"black\",\n",
    "}\n",
    "\n",
    "data_frames = []\n",
    "\n",
    "for i in range(int(n)):\n",
    "    data_frames.append(resultsdf[resultsdf.cluster == i])\n",
    "    plt.scatter(data_frames[i].entropy,data_frames[i][\"log-likelihood\"],color=colour_dict[i])\n",
    "\n",
    "#df1 = resultsdf[resultsdf.cluster == 0]\n",
    "#df2 = resultsdf[resultsdf.cluster == 1]\n",
    "#df3 = resultsdf[resultsdf.cluster == 2]\n",
    "#df4 = resultsdf[resultsdf.cluster == 3]\n",
    "\n",
    "#plt.scatter(df1.entropy,df1[\"log-likelihood\"],color=\"blue\")\n",
    "#plt.scatter(df2.entropy,df2[\"log-likelihood\"],color=\"red\")\n",
    "#plt.scatter(df3.entropy,df3[\"log-likelihood\"],color=\"green\")\n",
    "#plt.scatter(df4.entropy,df4[\"log-likelihood\"],color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ea079",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' It is possible that the user will not be happy with how the data has been assigned.\n",
    "Offer an option for them to redo the clustering . '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af35196a",
   "metadata": {},
   "source": [
    "### We now need to select the median  model (based on entropy) from each of these clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f780ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a list of models and their accuracies:\n",
    "\n",
    "models = {}\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "#Can't use median function because it finds an average for even sets\n",
    "\n",
    "for i in range(int(n)):\n",
    "    index = int(data_frames[i].shape[0]/2)\n",
    "    observation = data_frames[i].iloc[index]\n",
    "    #kernel = float(observation['const'])*RBF(float(observation['length_scale']))\n",
    "    #gp = GaussianProcessClassifier(kernel=kernel,optimizer=None).fit(X, Y)\n",
    "    models['model{}'.format(i)] = [observation['const'],observation['length_scale']]\n",
    "    accuracies.append(observation['log-likelihood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d55c8c5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-a225b886739c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#We can pull out the parameters we need\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#We can pull out the parameters we need\n",
    "\n",
    "print(models.get(\"model1\")[0])\n",
    "print(models.get(\"model1\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90d51fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be our PIGEBaQ algorithm:\n",
    "\n",
    "import math\n",
    "\n",
    "def compute_zn(index,data,output_var,np_lengthscale,np_cov,np_mu):\n",
    "    x = np.array(data.iloc[[index]])\n",
    "\n",
    "    element1 = (output_var*math.sqrt(np.linalg.det(np_lengthscale)))/(math.sqrt(np.linalg.det(np_lengthscale+np_cov)))\n",
    "    element2 = np.linalg.inv(np_lengthscale+np_cov)\n",
    "    element3 = np.transpose(np_mu - x)\n",
    "    element4 = -0.5*np.matmul(np.transpose(-1*element3),np.matmul(element2,-1*element3))\n",
    "\n",
    "    zn = element1*math.exp(element4.item())*np.matmul(element2,element3)\n",
    "    return zn\n",
    "\n",
    "def compute_cov(x,y,var,l):  #x and y are each full observations (vectors)\n",
    "    d = 0\n",
    "    m = len(x)\n",
    "    for i in range(m):\n",
    "        d += (x[i]-y[i])**2\n",
    "    val = var*np.exp(-0.5*d/(l**2))\n",
    "    return val\n",
    "\n",
    "\n",
    "def PIGEBaQ(data,var,length):\n",
    "    \n",
    "    this_df = data.dropna()\n",
    "    this_df.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    mu = list(this_df.mean(axis='index'))\n",
    "    del mu[-1]\n",
    "    m = len(mu)\n",
    "    n = len(this_df.index)\n",
    "    np_mu = np.array(mu)\n",
    "    \n",
    "    this_df2 = this_df.drop(columns=['Output'])\n",
    "    df_cov = this_df2.cov()\n",
    "    np_cov = df_cov.to_numpy()\n",
    "    \n",
    "    np_lengthscale = length*np.identity(m)\n",
    "    \n",
    "    Z = [0]*n\n",
    "    for i in range(0,n):\n",
    "        Z[i] = compute_zn(i,this_df2,var,np_lengthscale,np_cov,np_mu)\n",
    "    \n",
    "    Z = np.array(Z)\n",
    "    Z = np.transpose(Z)\n",
    "    Z = Z.reshape(m,n)\n",
    "    \n",
    "    X = this_df2.to_numpy() #Each row is an abservation, each column is a variable\n",
    "    \n",
    "    num = this_df2.shape[0]\n",
    "    K = np.empty([num,num])\n",
    "\n",
    "    #Now lets start filling this matrix:\n",
    "    \n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            K[i,j] = compute_cov(X[i,:],X[j,:],var,length)\n",
    "    \n",
    "    f = this_df[\"Output\"].to_numpy()\n",
    "    f = f.reshape(n)\n",
    "    \n",
    "    E = np.matmul(Z,np.matmul(np.linalg.inv(K),f))\n",
    "    \n",
    "    output_df = pd.DataFrame({\"Skills\":this_df2.columns,\"Importance\":E})\n",
    "    output_df = output_df.sort_values(by=\"Importance\")\n",
    "    output_df.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "    output_df['Importance'] = output_df['Importance']/math.sqrt(output_df['Importance'].pow(2).sum()) #Normalizes values\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7e7f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data_PIG = scaled_total_set.drop(columns=['O*NET-SOC Code', 'Title'])\n",
    "input_data_PIG = scaled_total_set.copy()\n",
    "\n",
    "tables = []\n",
    "\n",
    "for i in range(int(n)):\n",
    "    tables.append(PIGEBaQ(input_data_PIG,models.get(\"model{}\".format(i))[0],models.get(\"model{}\".format(i))[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea9dddad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skills</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.334570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.153344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.082758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.926121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Skills  Importance\n",
       "0      1   -0.334570\n",
       "1      2   -0.153344\n",
       "2      3    0.082758\n",
       "3      0    0.926121"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bcef37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " <ipython-input-34-2c5ce980b0df>:13: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.334570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.153344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.082758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.926121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Features  Importance\n",
       "0        1   -0.334570\n",
       "1        2   -0.153344\n",
       "2        3    0.082758\n",
       "3        0    0.926121"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We shall now generate a single dataframe which has the weighted explanations:\n",
    "\n",
    "explanations_df = pd.DataFrame({\"Features\":np.array(scaled_total_set.columns[0:4])})\n",
    "explanations_df[\"Importance\"] = 0.0\n",
    "#explanations_df\n",
    "\n",
    "model = tables[0] #This is the relevant importance dataframe\n",
    "\n",
    "#explanations_df[\"Importance\"][0] = (accuracies[0]/sum(accuracies))*model['Importance'][model.index[model['Skills'] == explanations_df['Features'][0]]]\n",
    "for j in range(int(n)):\n",
    "    model = tables[j]\n",
    "    for i in range(int(explanations_df.shape[0])):\n",
    "        explanations_df[\"Importance\"][i] += (accuracies[j]/sum(accuracies))*model['Importance'][model.index[model['Skills'] == explanations_df['Features'][i]]]\n",
    "\n",
    "\n",
    "explanations_df = explanations_df.sort_values(by=[\"Importance\"],ascending=True)\n",
    "explanations_df.reset_index(drop = True, inplace = True)\n",
    "explanations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4019a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations_df.to_csv(\"C:/Users/jacob/Documents/4YP data/DataSynthesis/Regression/4x400/interpretation.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbca202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explanations_df.to_csv(\"perm_PIGEBaQ.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the moment, we shall generate an explanation based on feature importance across the different clusters:\n",
    "\n",
    "\n",
    "tables = []\n",
    "\n",
    "for i in range(int(n)):\n",
    "    tables.append(pd.DataFrame({\"Features\":np.array(training_set.columns[2:37])}))\n",
    "\n",
    "\n",
    "for i in range(int(n)):\n",
    "    r = permutation_importance(models[i],X_test,y_test,n_repeats=30,random_state=0)\n",
    "    #tables[i][\"Importances\"] = abs(r.importances_mean)\n",
    "    tables[i][\"Importances\"] = r.importances_mean\n",
    "    tables[i] = tables[i].sort_values(by=[\"Importances\"],ascending=False)\n",
    "    tables[i].reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "#for i in range(int(n)):\n",
    "    #r = permutation_importance(models[i],X_test,y_test,n_repeats=30,random_state=0)\n",
    "    #importance_table[\"Importances{}\".format(i)] = abs(r.importances_mean)\n",
    "    #importance_table = importance_table.sort_values(by=['Importances{}'.format(i)])\n",
    "\n",
    "#NOTE - I'm uneasy about using the same test sets as before, should I expand to the unknown jobs?\n",
    "\n",
    "importance_dict = {}\n",
    "for i in range(int(n)):\n",
    "    q = tables[i]\n",
    "    importance_dict[\"Importance{}\".format(i)] = q[\"Features\"]\n",
    "#q = tables[1]\n",
    "#importance_dict[\"Importance1\"] = q[\"Features\"]\n",
    "importance_table = pd.DataFrame(data=importance_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d12e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importance_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec713c4",
   "metadata": {},
   "source": [
    "### We now have a table of features sorted by how important they are for each cluster, based on the permutation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4994965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shall generate one more table, which will have a row for each feature and a score for how important it is:\n",
    "\n",
    "final_importance_table = pd.DataFrame({\"Features\":np.array(training_set.columns[2:37]),\"Importance\":np.zeros(np.shape(training_set.columns[2]))})\n",
    "feature_list = final_importance_table['Features'].tolist()\n",
    "#final_importance_table.head()\n",
    "for i in feature_list:\n",
    "    for j in range(int(n)):\n",
    "        index_val = importance_table.index[importance_table[\"Importance{}\".format(j)]==i]\n",
    "        index_val = index_val*accuracies[j]/sum(accuracies)\n",
    "        final_importance_table.loc[final_importance_table[\"Features\"]==i,\"Importance\"] += index_val.tolist()[0]\n",
    "    \n",
    "final_importance_table = final_importance_table.sort_values(by=[\"Importance\"],ascending=True)\n",
    "final_importance_table.reset_index(drop = True, inplace = True)\n",
    "final_importance_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e26c97",
   "metadata": {},
   "source": [
    "### We now have a table which ranks features by importance across all the clusters! Note that is values each cluster equally - this might be something we can improve upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1470c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets divert our attention towards making predictions on our unknown jobs:\n",
    "\n",
    "unknown_jobs = df4[df4.isna().any(axis=1)] #This is our set of unknown jobs\n",
    "#SCALE THE DATA! - Do I need to fit a new scaler?\n",
    "X = scaler.transform(unknown_jobs.iloc[:,2:37])\n",
    "#X = np.array(unknown_jobs.iloc[:,2:37])\n",
    "print(X)\n",
    "#for i in int(n):\n",
    "    #models[i].predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = gpc.predict_proba(X)\n",
    "test1 = gpc.predict(X)\n",
    "test2 = gpc.predict_proba(X)\n",
    "print(test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now fill in the dataframe:\n",
    "\n",
    "unknown_jobs[\"Auto label value\"] = test1\n",
    "unknown_jobs[\"Auto probability\"] = test2[:,1]\n",
    "unknown_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b143b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_jobs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets think about how we can apply the model from each of our clusters and compute an average:\n",
    "\n",
    "for i in range(int(n)):\n",
    "    probs = models[i].predict_proba(X)\n",
    "    unknown_jobs[\"Auto probability{}\".format(i)] = probs[:,1]\n",
    "\n",
    "#We need a list of column titles:\n",
    "column_titles = []\n",
    "for i in range(int(n)):\n",
    "    column_titles.append(\"Auto probability{}\".format(i))\n",
    "\n",
    "#Now calculate mean automotability    \n",
    "\n",
    "unknown_jobs[\"Auto probability\"] = unknown_jobs[column_titles].mean(axis=1)\n",
    "\n",
    "#And finally apply a function to determine auto-label value:\n",
    "\n",
    "def label_auto(my_input):\n",
    "    if my_input - 0.5 < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "unknown_jobs[\"Auto label value\"] = unknown_jobs[\"Auto probability\"].apply(label_auto)\n",
    "\n",
    "unknown_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d836b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unknown_jobs[\"Title\"][\"i\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475056f",
   "metadata": {},
   "source": [
    "### We have now generated values for automotability!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
